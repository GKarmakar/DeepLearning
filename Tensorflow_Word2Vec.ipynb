{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: Image and audio processing data are rich high dimensional data encodede as vectors of the individual raw    pixels. Similary much of the information within natural language texts or speech are encoded as vectors of high dimensionality.But typical problem with traditional approach to extract useful information from natural language has been treatingeach word as discrete symbols and assigning ID to them. Then problem arises that the knowledge gained from the understanding of the word 'CAT' doesn't help to understand word 'DOG' although they belong to same class as 'ANIMAL' and shares many characteristic.Also traditonal method like Bag of Words and count based methods such as N-Gram is to deal with high dimensioanlity which is the total vocabulary size of the text and it could be in size of many millions. One other problem is to deal with any new vocab additions is complex. These methods also fails to capture all the semantic similarity of every words or phrases (e.g. wonderful & awesome) and how they exists in certain texts. Moreover, they are very hard to be robust to many different problems like machine translation, speech recognition and language understanding tasks.\n",
    "Representing words as vectors can overcome many of these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Space Models (VSM) embeds words as vectors and has a long reach history in NLP tasks. It shars distibutional hypothesis .\n",
    "In this hypothesis it is identified that words that appear together shares same semantic meaning or context. The different\n",
    "approach that applies this hypothesis are Latent Semantic Analysis (LSA - Count based method) and Neural Probabilistic Method\n",
    "(Precitive).\n",
    "One of the popular method currently is word2vec which follows the principal of distributional similarity based representation.\n",
    "There is a great saying in this regard and I quote, \"You shall know a word by the company it keeps\". Word is represented \n",
    "by the left and right words of a window with a size. We will explore this in greater details later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec is particularly efficient in terms of computation because it works with a window for co-occurance from raw text.\n",
    "It comes in two flavor - Continuous Bag of Words and Skip Gram Model (Mikolov et. al.).Algorithimically these are same except that CBOW predicts the next word based on a context and Skip-Gram predicts the context based on a word. CBOW treats each context as one observation and smoothes over a lot of distributional information but skip gram treats every context-target as new observation and hence performs better for large datasets.\n",
    "\n",
    "Traditionally probabilistic model uses MLE method to optimize but normalize over entrire vocab is expensive and word2vec doesn't use full probabilstic model rather uses a technique called negative sampling where it tries to maximize logprobability of correct word appearing in the context and minimize noise from random sampled words from the rest of the corpus not appearning in the context.\n",
    "\n",
    "Mathametically speaking:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified: text8.zip\n"
     ]
    }
   ],
   "source": [
    "#dowload the data\n",
    "url = \"http://mathmahoney.net/dc/\"\n",
    "\n",
    "def download_data(filename, expected_bytes):\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified:', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify;' + filename + '. Can you get it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = download_data('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "#read the data into list of strings\n",
    "\n",
    "def read_data(filename):\n",
    "    #Extract first file in the zip as a list of words\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build the dictionary and replace the rare words with UNK token\n",
    "\n",
    "vocab_size = 50000\n",
    "\n",
    "def build_dataset(words, vocab_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size -1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count +=1 \n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+unk) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(words, vocab_size)\n",
    "#del words # to reduce memory\n",
    "print(\"Most common words (+unk)\", count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "#Step -3: Function to generate a training batch for the skip-gram model\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [skip_window target skip_window]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window # target level at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span -1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "        #Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "        data_index = (data_index + len(data) - span) % len(data)\n",
    "        return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3084 originated -> 5239 anarchism\n",
      "3084 originated -> 12 as\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "268787658",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c56aa2dcc39d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     print(batch[i], reverse_dictionary[batch[i]], \n\u001b[0;32m----> 4\u001b[0;31m           '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 268787658"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(batch_size = 8, num_skips = 2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], \n",
    "          '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "embedding_size = 128 # Dimension of embedding vectors\n",
    "skip_window = 1 #How many words to consider left and right\n",
    "num_skips = 2 # How many times to reuse an input to generate a label\n",
    "\n",
    "#We pick a random validation set to sample nearest neighbos, here we limit the \n",
    "#validation samples to the words to the words that have low numeric ID which by construction are also\n",
    "#most frequent words\n",
    "\n",
    "valid_size = 16 #random set of words to evaluate simimilarity\n",
    "va,id_window = 100 # Pick up samples \n",
    "valid_exeamples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "\n",
    "with graph as default():\n",
    "    \n",
    "    #Input data\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtypetf.int32)\n",
    "    \n",
    "    #Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        #Look up embeddings for inputs\n",
    "        embeddings = tf.Variable(\n",
    "             tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        #Construct the variable for the NCE loss\n",
    "        nce_weight = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        nce_bias = tf.Variable(tf.zeros([vocab_size]))\n",
    "        \n",
    "        \n",
    "        #Compute the average NCE loss for the batch\n",
    "        #tf.nce_loss automatically drew as new sample of the negative labels\n",
    "        #each time we evaluate the loss\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                             biases = nce_bias,\n",
    "                                             labels=train_labels,\n",
    "                                             inputs = embed,\n",
    "                                             num_smapled=num_sampled,\n",
    "                                             num_classes = vocab_size))\n",
    "        \n",
    "        #Construct the sgd optimizer using a learning rate of 1.0\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        \n",
    "        #Compute the cosien similarity between minibatch examples and all other embeddings\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_mean(tf.squre(embeddings), 1, keep_dims=True))\n",
    "        \n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "                 normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        \n",
    "        #Add variable initializer\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        #Begin training\n",
    "        \n",
    "        num_steps = 100001\n",
    "        \n",
    "        with tf.Session(graph=graph) as session:\n",
    "            #We must initialize all variables before we use them\n",
    "            init.run()\n",
    "            print('Initialized')\n",
    "            \n",
    "            \n",
    "            average_loss = 0\n",
    "            for step in range(num_steps):\n",
    "                batch_size, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "                feed_dict={train_inputs: batch_inputs, train_labels: batch_labels}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
